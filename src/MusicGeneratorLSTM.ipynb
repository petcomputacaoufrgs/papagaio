{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MusicGeneratorLSTM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOxDybNd0ZnJX43i12e9rCY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e4991997ce864fc7910abc99a373628b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_14901f49828146ef9805c8e4d7ebfa0a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4286e1efe97d489ba7e9f8b65243f1e9",
              "IPY_MODEL_750b0f205a7a46d69fad654f488447be"
            ]
          }
        },
        "14901f49828146ef9805c8e4d7ebfa0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4286e1efe97d489ba7e9f8b65243f1e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0772273cfeff463e89b2c7e9c4d41f30",
            "_dom_classes": [],
            "description": "  2%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 5010,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 88,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_90a5575eeb044d1d88c97f385e81a8b6"
          }
        },
        "750b0f205a7a46d69fad654f488447be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_56799a20085342f4908038ca488afb46",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 88/5010 [00:26&lt;28:23,  2.89it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_34c3d700feed4fa5aeddf4b9126d4196"
          }
        },
        "0772273cfeff463e89b2c7e9c4d41f30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "90a5575eeb044d1d88c97f385e81a8b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "56799a20085342f4908038ca488afb46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "34c3d700feed4fa5aeddf4b9126d4196": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edufantini/music-gen/blob/main/src/MusicGeneratorLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wkARadSlGB2"
      },
      "source": [
        "# Music Generator with LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5hn1sHZlMEK"
      },
      "source": [
        "##About"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmmZd52Ullwl"
      },
      "source": [
        "Music resamples language as a temporal sequence of articulated sounds. They say something, often something human.\n",
        "\n",
        "Although, there are crucial differences between language and music. We can still describe it as a sequence of symbols in the simplest form of understanding. Translating something complex into something simpler, but usable by computational models.\n",
        "\n",
        "Thus, the objective of this project is to establish a communication between the human, that understands music in the most intense way that the brain can interpret through information, and the machine.\n",
        "\n",
        "We'll create a model that can generate music based on the input information, i.e., generate a sequence of sounds which are related in some way with the sounds passed as input.\n",
        "\n",
        "We'll use Natural Language Processing (NLP) methods, observing the music as it were a language, abstracting it. Doing this, the machine can recognize and process similar data.\n",
        "\n",
        "On the first step, we'll use text generation techniques, using Recurrent Neural Networks (RNNs) and Long-Short Term Memories (LSTMs). With the effectiveness of the training, even if it's reasonable, we'll perform the same implementation using specific methods such as Attention.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi_qPalLptT_"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COTQXLo7W4yj"
      },
      "source": [
        "# Basic libraries\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Preprocessing data libraries\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Model libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Data visualization\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm.notebook import tqdm #for loading bars"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z66wYkcg42SL"
      },
      "source": [
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0Oql7yYrLCA",
        "outputId": "47554864-7412-4ec5-d739-44f7f07a6fee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/edufantini/music-gen.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'music-gen'...\n",
            "remote: Enumerating objects: 19514, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 19514 (delta 15), reused 45 (delta 14), pack-reused 19468\u001b[K\n",
            "Receiving objects: 100% (19514/19514), 221.29 MiB | 21.06 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXlV5taYsaWQ"
      },
      "source": [
        "from music_gen.src.GetData import *"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKgKH6V3pU6J"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxtDWPuJqf7Y",
        "outputId": "0746b3f8-0126-407a-c5cb-2e86f45a6aee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVrI9CSVql2b"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAGfOBGMqngC",
        "outputId": "05c4cedb-cbad-40be-c444-79b646ffa1d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/gdrive/My Drive/Kaggle"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Kaggle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEDTESw4qqES",
        "outputId": "0753274d-fa2d-4014-c970-c0a95fdb907d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!kaggle datasets download -d edufantini/songs-in-midi"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "songs-in-midi.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLcHEoClqsCm",
        "outputId": "bea33def-2db8-4c0c-928e-421fd9025203",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clean_midi  dataset  kaggle.json  songs-in-midi.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okzKx-JwqtSZ"
      },
      "source": [
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRYkyNh0XuQF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "bcc2ff98-72fb-4f00-bff4-faced4006eb6"
      },
      "source": [
        "path = '/content/gdrive/My Drive/Kaggle/clean_midi/AC_DC/'\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for filename in os.listdir(path):\n",
        "  if filename.endswith(\"mid\"): \n",
        "    # Your code comes here such as \n",
        "    print(path + filename)\n",
        "    #if filename is not 'Back_In_Black.mid':\n",
        "    data = encode_data(path+filename, 32)\n",
        "    dataset.append(data)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Kaggle/clean_midi/AC_DC/Dirty_Deeds_Done_Dirt_Cheap.mid\n",
            "Processing file /content/gdrive/My Drive/Kaggle/clean_midi/AC_DC/Dirty_Deeds_Done_Dirt_Cheap.mid\n",
            "Processing part 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Converting measures from part 1: 100%|###############################################################################| 58/58 [00:00<00:00, 290.23it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processing part 2/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rConverting measures from part 2:   0%|                                                                                         | 0/63 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rConverting measures from part 2:   0%|                                                                                         | 0/63 [00:02<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c9c294685751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#if filename is not 'Back_In_Black.mid':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/music_gen/src/GetData.py\u001b[0m in \u001b[0;36mencode_data\u001b[0;34m(path, n_frames)\u001b[0m\n\u001b[1;32m    136\u001b[0m         for it in tqdm(part.measures(1, len(data)),\n\u001b[1;32m    137\u001b[0m                        \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Converting measures from part {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                        ascii=True, ncols=150):\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstrument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInstrument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI53Zcgucr6i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d94e3c03-a22d-4ccd-b5c0-a57cedc4fe6e"
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xwZHHmu2qRQ"
      },
      "source": [
        "## Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iig3c90b2vmP"
      },
      "source": [
        "```preprocess_bar(encoded_seq, n_in=32, n_out=32)``` uma barra (32 frames) codificada em multi-hot pre-processa essa barra de forma a gerar os valores splitados em X e y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qerBewo5nZsX"
      },
      "source": [
        "def preprocess_bar(encoded_seq, n_in=32, n_out=32):\n",
        "  # create lag copies of the sequence\n",
        "  df = pd.DataFrame(encoded_seq)\n",
        "  df = pd.concat([df.shift(n_in-i-1) for i in range(n_in)], axis=1)\n",
        "  # drop rows with missing values\n",
        "  df.dropna(inplace=True)\n",
        "  # specify comumns for inout and output values\n",
        "  values = df.values\n",
        "  width = encoded_seq.shape[1]\n",
        "  X = values[:, 0:width*(n_in-1)].reshape(n_in-1, width)\n",
        "  y = values[:, width:].reshape(n_in-1, width)\n",
        "  return X,y"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaW7S-QNFp_0"
      },
      "source": [
        "```create_dataloader(dataset, batch_size=1)``` converte um dataset com n musicas em um dataloader. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-AdhDdjlwaP"
      },
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def create_dataloader(dataset, batch_size=1):\n",
        "  X = []\n",
        "  y = []\n",
        "  n_songs = 0\n",
        "  n_parts= 0\n",
        "  n_bars= 0\n",
        "\n",
        "  # create a two arrays X, y with bars\n",
        "  for song in dataset:\n",
        "    for part in song:\n",
        "      for bar in part:\n",
        "        xa, ya = preprocess_bar(bar)\n",
        "        X.append(xa)\n",
        "        y.append(ya)\n",
        "\n",
        "  X = np.array(X)\n",
        "  y = np.array(y)\n",
        "  X = torch.from_numpy(X)\n",
        "  y = torch.from_numpy(y)\n",
        "  print(X.shape, y.shape)\n",
        "  train_ds = TensorDataset(X, y)\n",
        "  train_dl = DataLoader(train_ds, batch_size=1, shuffle=False)\n",
        "\n",
        "  return train_dl\n",
        "           "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RusAe2SNGZOx"
      },
      "source": [
        "O dataloader é dividido em duas partes:\n",
        "\n",
        "  1.   context\n",
        "  2.   target\n",
        "\n",
        "onde context são os valores que serão passados como entrada para o modelo - neste caso, esses valores serão 31 frames localizados em cada uma das 5010 barras das musicas e cada frame possui 88 notas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKg3d8D1pAiq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60caba51-7670-4514-a8c7-95a9cc879ff9"
      },
      "source": [
        "train_dl = create_dataloader(dataset)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5010, 31, 88]) torch.Size([5010, 31, 88])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImwNiACeuO2r"
      },
      "source": [
        "def create_vocab(dataset):\n",
        "  vocab = []\n",
        "  for song in dataset:\n",
        "    for part in song:\n",
        "      for bar in part:\n",
        "        vocab.append(bar)\n",
        "\n",
        "  vocab = np.array(vocab)\n",
        "  vocab = vocab.reshape(vocab.shape[0]*vocab.shape[1], vocab.shape[2])\n",
        "  vocab = np.unique(vocab, axis=0)\n",
        "\n",
        "\n",
        "  #np.set_printoptions(threshold=np.inf) print full array\n",
        "  #np.set_printoptions(threshold=10)\n",
        "  print(vocab)\n",
        "  \n",
        "  return vocab"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGlTzX66LELP"
      },
      "source": [
        "Temos 837 frames distintos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uCQtfMpKxW3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12ace32e-ad67-457b-8637-f9858d1c7166"
      },
      "source": [
        "diff_frames = create_vocab(dataset)\n",
        "print('\\nvocab len: {}'.format(diff_frames.shape[0]))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "\n",
            "vocab len: 626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36_oJxxUKqXV"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WoQLay4L2UH"
      },
      "source": [
        "Algumas definicoes importantes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dDRl36nL1g3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bff824c-8578-4d0f-b9f7-4e89d695117e"
      },
      "source": [
        "n_frames_input = 31\n",
        "n_frames_output = 31\n",
        "n_bars_input = len(train_dl.dataset.tensors[0]) # number of rows of the dataloader\n",
        "print('Number of bars in the input dataset: {}'.format(n_bars_input))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of bars in the input dataset: 5010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaGNDREuPPxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ad7f7b-377d-48d3-c904-1690971d0892"
      },
      "source": [
        "input = torch.zeros(n_bars_input, 31, 88)\n",
        "target = torch.zeros(n_bars_input, 31, 88)\n",
        "\n",
        "for sample, (xb, yb) in enumerate(train_dl): # gets the samples\n",
        "  input[sample] = xb\n",
        "  target[sample] = yb\n",
        "\n",
        "input[1, 1, :].shape, target[1, 1, :].shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([88]), torch.Size([88]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xccNf_olwOB-"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "    super(RNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    #self.embed = nn.Embedding(input_size, hidden_size)\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=False)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "    self.act = nn.Hardsigmoid()\n",
        "\n",
        "  def forward(self, x, hidden, cell):\n",
        "\n",
        "    # print(\"\\n\\nX \\t\", x.unsqueeze(1).shape)\n",
        "\n",
        "    # Passing in the input and hidden state into the model and obtaining outputs\n",
        "    out, (hidden, cell) = self.lstm(x.unsqueeze(1), (hidden, cell))\n",
        "\n",
        "    # print(\"\\n\\nX, out, hidden, cell\\t\", x.shape, out.shape, hidden.shape, cell.shape)\n",
        "\n",
        "    # Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "    out = self.fc(out.contiguous().view(-1, self.hidden_size))\n",
        "    out = self.act(out)\n",
        "\n",
        "    # print(\"\\n\\nOut, hidden\\t\", out.shape, hidden.shape)\n",
        "    return out, (hidden, cell)\n",
        "\n",
        "  # CHECK!!!!!!\n",
        "  def init_hidden(self, batch_size):\n",
        "    # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
        "    # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
        "    hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "    cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "    return hidden, cell"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDmbAnLoM8Ua"
      },
      "source": [
        "A classe ```Generator()``` realiza as operacoes principais de treino e teste da LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvmgiYUvM8p0"
      },
      "source": [
        "class Generator():\n",
        "  def __init__(self):\n",
        " \n",
        "    # preparation\n",
        " \n",
        "    self.bar_len = 31 # how many frames it's gonna take in a timestep\n",
        "    self.num_layers = self.bar_len\n",
        " \n",
        "    self.frame_len = 88\n",
        "    self.hidden_size = self.frame_len\n",
        " \n",
        "    self.num_epochs = 5\n",
        "    self.batch_size = 1 # ou 31?\n",
        " \n",
        "    self.lr = 0.003\n",
        " \n",
        " \n",
        " \n",
        "  # converts one frame into torch tensor\n",
        "  def multi_hot_tensor(self, frame):\n",
        "    tensor = torch.from_numpy(frame)\n",
        "    return tensor\n",
        " \n",
        " \n",
        "  # retrieve data from dataloader\n",
        "  def get_sample(self, dataloader):\n",
        " \n",
        "    input = torch.zeros(n_bars_input, self.bar_len, self.frame_len)\n",
        "    target = torch.zeros(n_bars_input, self.bar_len, self.frame_len)\n",
        " \n",
        "    for sample, (xb, yb) in enumerate(dataloader): # gets the samples\n",
        "      input[sample] = xb\n",
        "      target[sample] = yb\n",
        "    \n",
        "    return input, target\n",
        " \n",
        " \n",
        " \n",
        "  def generate(self, initial_bar, predict_len, temperature=0.85):\n",
        "    pass\n",
        " \n",
        " \n",
        " \n",
        "  def train(self, dataloader):\n",
        " \n",
        "    # Instantiate the model with hyperparameters\n",
        "    # We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "    self.rnn = RNN(input_size=self.frame_len,\n",
        "                   output_size=self.frame_len,\n",
        "                   hidden_size=self.hidden_size,\n",
        "                   num_layers=self.num_layers).to(device)\n",
        " \n",
        " \n",
        "    optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n",
        " \n",
        "    # podemos?\n",
        "    loss_fn = nn.BCELoss() # alterar loss -> cross entropy\n",
        " \n",
        " \n",
        "    print(\"\\nStarting training...\")\n",
        " \n",
        " \n",
        "    loss = 0\n",
        "    for epoch in range(1, self.num_epochs + 1):\n",
        " \n",
        "      print('> EPOCH #', epoch)\n",
        " \n",
        "      # generates the predictions\n",
        " \n",
        "      input, target = self.get_sample(dataloader)\n",
        "      hidden, cell = self.rnn.init_hidden(self.batch_size)\n",
        " \n",
        "      input = input.to(device)\n",
        " \n",
        "      target = target.to(device)\n",
        " \n",
        "      # print(\"\\n\\t Input:\\t\\t\", input.shape)\n",
        " \n",
        "      for bar in tqdm(range(n_bars_input)):\n",
        "        hidden, cell = self.rnn.init_hidden(self.batch_size)\n",
        " \n",
        "        # print(\"\\n\\t Hidden:\\t\", hidden.shape)\n",
        "        # print(\"\\t Cell:\\t\\t\", cell.shape)\n",
        " \n",
        "        output, (hidden, cell) = self.rnn(input[bar,:], hidden, cell)\n",
        " \n",
        "        # print(\"\\n\\t Output shape:\\t\\t\", output.shape)\n",
        "        # print(\"\\t Target shape:\\t\\t\", target[bar, :].shape)\n",
        " \n",
        "        #for f in range(output.shape[0]):\n",
        " \n",
        "        #np.set_printoptions(threshold=np.inf)\n",
        "        #print('\\n\\t Output:\\n\\t', output.cpu().detach().numpy())\n",
        "        #np.set_printoptions(threshold=10)\n",
        "\n",
        "        # print('\\n\\t Target:\\n\\t', target[bar, :])           \n",
        "        loss_step = loss_fn(output, target[bar, :])\n",
        "        #loss += loss_step\n",
        "        #print('\\n\\t Loss step:\\t', loss_step)\n",
        "        #print('\\t Loss:\\t\\t', loss)\n",
        "        loss_step.backward() # Does backpropagation and calculates gradients\n",
        "        optimizer.step() # Updates the weights accordingly\n",
        "        optimizer.zero_grad() # Clears existing gradients from previous frame\n",
        "        #loss_step = 0\n",
        "        \n",
        "      if epoch%10 == 0:\n",
        "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "        print(\"Loss: {:.4f}\".format(loss.item()))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYalmBKwSOFj"
      },
      "source": [
        "gen = Generator()"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytEk9CSYSUVc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390,
          "referenced_widgets": [
            "e4991997ce864fc7910abc99a373628b",
            "14901f49828146ef9805c8e4d7ebfa0a",
            "4286e1efe97d489ba7e9f8b65243f1e9",
            "750b0f205a7a46d69fad654f488447be",
            "0772273cfeff463e89b2c7e9c4d41f30",
            "90a5575eeb044d1d88c97f385e81a8b6",
            "56799a20085342f4908038ca488afb46",
            "34c3d700feed4fa5aeddf4b9126d4196"
          ]
        },
        "outputId": "4543cad7-2873-4cd5-f055-21bfefbaa7f5"
      },
      "source": [
        "gen.train(train_dl)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n",
            "> EPOCH # 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4991997ce864fc7910abc99a373628b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=5010.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-bd40a16bf42b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-473ec40b18af>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m#print('\\n\\t Loss step:\\t', loss_step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m#print('\\t Loss:\\t\\t', loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mloss_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Does backpropagation and calculates gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Updates the weights accordingly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Clears existing gradients from previous frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}